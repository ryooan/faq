---

---

# Prediction Resources

In order to be the best possible predictor — to climb to the top of the [rankings](https://www.metaculus.com/rankings/) and establish yourself as a _Metaculus Time Lord_ — you'll need to gather as much data as possible, filter out all the noise, and distill it down to a series of insightful projections that can pinpoint the future with laser-like accuracy. It's not an easy task, but by making smart use of the resources presented here you should be well on your way.

Please note that this page is a continual work in progress! If you have a useful resource to add, let us know in the [associated discussion comments](https://www.metaculus.com/questions/1073/what-are-the-most-useful-toolsdata-sources-in-making-predictions/) or send us a note at [support@metaculus.com](mailto:support@metaculus.com).

## Analysis tools

*   **[Guesstimate](https://www.getguesstimate.com/)**: a simple web-based tool to model uncertainties in calculations. Guesstimate's interface is similar to other spreadsheet tools, such as Excel or Google Sheets. Each model is a grid of cells, and each cell can be filled with a name and value. Functions can be used to connect cells together to represent more complex quantities.  

    For example, consider the [question series](https://www.metaculus.com/questions/1337) about the Fermi paradox. We may use the Drake equation (a "back of the envelope" estimation to find out if there is intelligent life in the Milky Way other than us humans) to estimate the number of intelligent civilizations in our milky verse based on 7 different variables (see[drake equation](https://en.wikipedia.org/wiki/Drake_equation)). Each guess has its own uncertainties, and with Guesstimate you can multiply the guesses and their uncertainties together to get a probability distribution of the number of intelligent civilizations. See the following [model by a Guesstimate user on this probability](https://www.getguesstimate.com/models/2734 ). Also check out [public models](https://www.getguesstimate.com/models ), and **don't forget to post your models in the comments of questions for others to see!**

*   **Spreadsheets such as Excel or [Google Sheets](https://www.google.com/sheets/about/ )** for both theoretical modelling and basic statistical analysis. Spreadsheets offer similar options to Guesstimate, as you can create theoretical models to factorize questions, produce estimates for subquestions, and run basic Monte Carlo simulations (see [here](https://www.youtube.com/watch?v=Nb63swYetzY ) for an example of such simulation). Secondly, basic statistical analysis (descriptive statistics, correlations, regressions and so on) is convenient in Excel (see [here](https://ire.org/media/uploads/car2013_tipsheets/excel_stats_nicar2013.pdf ) for more information). Finally, spreadsheets created on [Google Sheets](https://www.google.com/sheets/about/ ) can also be shared in the comments, to allow others to view your work.

*   **Statistical Software**, like [R](https://www.r-project.org/ ), for more advanced statistical computing (linear and nonlinear modeling, classic statistical tests, time-series analysis, classification, clustering) and graphics. You can download it [here](http://cran.us.r-project.org/ ) for free.

*   **Probability Distribution Calculators** such as the [Normal distribution calculator](https://stattrek.com/online-calculator/normal.aspx ), the [Binomial distribution calculator](https://stattrek.com/online-calculator/binomial.aspx ), and the [Poisson distribution calculator](https://stattrek.com/online-calculator/poisson.aspx ). Lastly, check out this [Bayes Rule Calculator](https://stattrek.com/online-calculator/bayes-rule-calculator.aspx ) for updating your credence for yes/no questions given new information.

*   **[HASH](https://hash.ai/ )**: System modeling software can generate and inform forecasts of complex systems. HASH can be used to represent complex systems and run "what-if " scenarios, to hone your intuitions and improve your predictions.

## Tutorials, textbooks and other resources

*   Join [Replication Markets](https://www.replicationmarkets.com/ ), a contest where users forecast whether various studies will replicate. This is a great way to hone your prediction skills, to further behavioral and social scientific knowledge, and to maybe even take home some of the $100,000 in prize money.

*   Play [Calibrate Your Judgment](https://www.openphilanthropy.org/calibration?fbclid=IwAR2-__1Iz1jcb4gvHope66JRuDzz0WgHKki_KxIrIgxeap06flC3gT1NyJY ), an interactive calibration tutorial produced by the OpenPhilantropy Project. This is perhaps the most useful free online calibration training currently available. Note that you must sign in with a GuidedTrack, Facebook, or Google account, so that the application can track your performance over time.

*   AI Impact's [Evidence on good forecasting practices from the Good Judgment Project](https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project/ ) summarises the findings of the Good Judgment Project, the winning team in IARPA’s 2011-2015 forecasting tournament. The article describes the various correlates of successful forecasting as well as the heuristics, forecasting methodologies, philosophical outlooks, thinking styles that were associated with better predictions. Furthermore, it includes a helpful "recipe " for making predictions that describes how superforcasters (top 0.2% of forecasters) go about making their predictions.

*   [Forecasting: Principles and Practice](https://otexts.org/fpp2/index.html ) provides a comprehensive introduction to forecasting methods and present enough information about each method for readers to use them sensibly. The book is easy to read, is concise and presumes only basic statistics knowledge.  

    The book presents key concepts of forecasting. From [judgmental forecasting](https://otexts.org/fpp2/judgmental.html ) (which can be useful when you have no or few data) to [simple/multiple regression](https://otexts.org/fpp2/regression.html ), [time series decomposition](https://otexts.org/fpp2/decomposition.html ), [exponential smoothing (ETS)](https://otexts.org/fpp2/expsmooth.html ), and a few more advanced topics such as [Neural Networks](https://otexts.org/fpp2/nnetar.html ) (all in R). The book is optimised for providing useful advice on the making of predictions, and does not attempt to give a thorough discussion of the theoretical details behind each method.

*   [Open Textbooks on Forecasting and Related Courses by Francis Diebold](https://www.sas.upenn.edu/~fdiebold/Textbooks.html?fbclid=IwAR0ecodC_bGr8bxDtAgEy1ziiI8ohocH0S0IBE-Qvah3m46f1uVnNM3MtZo ), and especially his [Time-Series Econometrics: Forecasting](https://www.sas.upenn.edu/~fdiebold/Teaching221/econ221Penn.html ) , which provides an upper-level undergraduate / masters-level introduction to forecasting, broadly defined to include all aspects of predictive modeling, in economics and related fields. Having used this book for my macroeconometrics course, I highly recommend this book especially for the modelling of autogressive processes for making point and density forecasts (which are especially useful to numeric-range predictions on Metaculus).  

    The topics covered include: regression from a predictive viewpoint; conditional expectations vs. linear projections; decision environment and loss function; the forecast object, statement, horizon and information set; the parsimony principle, relationships among point, interval and density forecasts, and much more. The [book can be found here](https://www.sas.upenn.edu/~fdiebold/Teaching221/Forecasting.pdf ), and the [lecture slides covering material in the book can be found here](https://www.sas.upenn.edu/~fdiebold/Teaching221/Slides.pdf ). Diebold's resources are licensed under Creative Commons.

## Tips on how to become a better predictor

*   **Avoid overconfidence.** Overconfidence is a common finding in the forecasting research literature, and is found to be present in a 2016 analysis of [Metaculus predictions.](https://metaculus.wordpress.com/2016/10/15/analysis-of-20000-predictions/ ) Overconfidence comes in many forms, such as overconfidence in intuitive judgements, explicit models, or (your or other's) domain-specific expertise. Generally overconfidence leads people to:  


1. neglect decision aids or other assistance, thereby increasing the likelihood of a poor decision. In experimental studies of postdiction in which each were provided decision aids, subject-level expertise (and thereby confidence) was found to be correlated with lower levels of use of reliable decision aids, and [worse predictions overall](https://link.springer.com/content/pdf/10.1007/978-0-306-47630-3_22.pdf ).

2. make predictions contrary to the base rate. The base rate is the prevalence of a condition in the population under investigation. To expect the future to be substantially different from the past, one must have good evidence that i) some process crucial to bringing the usual result about will fail, and ii) the replacement process will produce a different outcome. Bayes rule teaches us that to predict unlikely events we must have highly diagnostic information (information that you'd be unlikely to observe in the usual case) whilst often predictors rely on their confidence rather than diagnosticity of evidence in going against the base rate.

    To counteract overconfidence forecasters should heed **five principles**: **(1)** Consider alternatives, especially in novel or unprecedented situations for which data is lacking; **(2)** List reasons why the forecast might be wrong; **(3)** In group interaction, appoint a devil’s advocate (or play the devil's advocate in the comment section!); **(4)** Obtain feedback about predictions (by posting it in the comments for example); **(5)** Treat the feedback you receive as valuable information.

*   **Break seemingly intractable problems into tractable sub-problems.** This is Fermi-style thinking. Enrico Fermi designed the first atomic reactor. When he wasn’t doing that he loved to tackle challenging questions such as “How many piano tuners are in Chicago?” At first glance, this seems very difficult. Fermi started by decomposing the problem into smaller parts and putting them into the buckets of knowable and unknowable. By working at a problem this way you expose what you don’t know or, as Tetlock (2016) puts it, you “flush ignorance into the open.”

*   **Discover the relevant base rate.** A _Metaculus time lord_ knows that there is nothing truly new under the sun. So, the best of forecasters often conduct creative searches for comparison classes even for seemingly unique events and pose the question: How often do things of this sort happen in situations of this sort? Identify comparison classes for events, and let your predictions be informed by the base-rate of occurrence in this class of events. This is often easier and more effective then it is to understand the event's working from first-principles.

*   **Combine systematic ‘model-thinking’ approach with an intuition-based approach**. Whilst it might be often good to use systematic ‘model-thinking’ approach that uses explicit theoretical or statistical reasoning, you should generally also use an intuition-based approach to predicting. When these two approaches yield different answers, think carefully about whether your question is the type of question that is better answered with intuitive judgments or with systematic modelling, and combine the two answers accordingly to inform your prediction. According to [Kahneman](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow ), intuitive judgements about some subject likely to be accurate only when the following three conditions hold:  

    *   The relevant subject exhibits a large degree of regularity
    *   One has had sufficient amount of exposure to this subject to have been able to pick up the relevant regularities
    *   One has received enough feedback to evaluate previous intuitive judgments

*   **Look for the errors behind your mistakes.** It’s easy to justify or rationalize your failure. Don’t. Own it and evaluate your track record (both resolution and calibration) and compare this the community track record. You want to learn where you went wrong and determine ways to get better. And don’t just look at failures. Evaluate successes as well so you can determine whether you used reliable techniques for producing forecasts or whether you were just plain lucky. For example, if you have an average log-score above 0.2, this might be evidence of overconfidence; in which case you should follow the tips on counteracting overconfidence presented above.

*   **Share your work in the question's comments section.** Sharing your theoretical reasoning (such as posting your Guesstimate model), statistical reasoning, information/data sources, or dependencies with others is good practice not just because you’re providing a valuable public good for our understanding of the future, but also because others may supplement your work with additional insight.

## Data Sources

### General Data Sources
*(in no particular order)*

|Data Service|Organization|Topics|Size|Ease of Use|Comments|
|--- |--- |--- |--- |--- |--- |
|[Public Data Explorer](https://www.google.com/publicdata/directory)|Google|All topics|Very large<br /><br />Public Data Explorer aggregates public data from113 dataset providers (such as international organizations, nationalstatistical offices, non-governmental organizations, and research institutions)|Very Easy<br /><br />This is a good place to start with your searchfor data, since many datasets are available which are often straightforward tofind. There are sometimes also great visualizations|This is perhaps the best place to look for public data and forecasts provided from third-party data providers<br /><br />Highly recommended also is the [International Futures Forecasting Data](https://www.google.com/publicdata/explore?ds=n4ff2muj8bh2a_) on long-term forecasting and global trend analysis available on the Public Data Explorer|
|[Our World in Data](https://ourworldindata.org/)|The Oxford Martin Programme on GlobalDevelopment at the University of Oxford|Global living conditions: Health, Food Provision, The Growth and Distribution of Incomes, Violence, Rights, Wars, Culture, Energy Use, Education, and Environmental Changes|Small<br /><br />Our World in Data aggregates some hundreds of datasets, all of which are organized well and given appropriate context|Very Easy<br /><br />There are excellent visualizations. Each topic the quality of the data is discussed and, by pointing the visitor to the sources, this website is also a database of databases. Covering all of these aspects in one resource makes it possible to understand how the observed long-run trends are interlinked|Highly recommended for big picture questions about the human condition|
|[Data.gov](https://data.gov/)|Various branches of the U.S. Government|Agriculture, Climate, Consumer, Education, Energy, Finance, Health, Manufacturing, Public Safety, Science and Research|Very Large<br /><br />Over 285,000 datasets from most federal departments, city governments, universities, NGOs and the private sector.|Moderately difficult<br /><br />You do need to enter in good search queries to get a short list of relevant results.|You can really find data on almost anything|
|[The World Bank Open Data](https://data.worldbank.org/)|The World Bank|Agriculture & Rural Development, Aid Effectiveness, Climate Change, Economy & Growth, Education, Energy & Mining, Environment, Financial Sector, Gender, Health, Infrastructure, Poverty, Science & Technology, Social Development, Trade, Urban Development|Large<br /><br />17,445 Datasets available|Easy|Their datasets on [Science & Technology](https://data.worldbank.org/topic/science-and-technology) might especially relevant for Metaculus questions|
|[UNData](http://data.un.org/)|United Nations Statistics Division|Agriculture, Crime, Education, Employment, Energy, Environment, Health, HIV/AIDS, Human Development, Industry, Information and Communication Technology, National Accounts, Population, Refugees, Tourism, Trade, as well as the Millennium Development Goals indicators|Large|Very Easy|Very intuitive interface for dataset searching|
|[Global Health Observatory](https://www.who.int/data/gho)|The World Health Organization|Health-related topics|Moderately large<br /><br />1000 indicators for its 194 member states|Easy<br /><br />You can browse this data by theme, [country](https://www.who.int/data/gho/data/countries), or [indicator](https://www.who.int/data/gho/data/indicators)|Excellent for health-related questions,such as those involving pandemics, antimicrobial resistance, and malaria|
|[OECDstat](https://stats.oecd.org/)|Organisation for Economic Co-operation and Development (OECD)|Technology and Patents, Development, Environment, Globalisation, Finance, Health, Industry, Information and Communication Technology, Productivity, Social Protection and Wellbeing,Transport, and more||Very easy<br /><br />Their online statistical database permits google-like keyword search||

### Macroeconomic & Finanical Only Data Sources
*(in no particular order)*

|Data Service|Organization|Topics|Size|Ease of Use|Comments|
|--- |--- |--- |--- |--- |--- |
|[Bureau of Economic Analysis](https://www.bea.gov/about)|U.S. Department of Commerce|Official macroeconomic and industry statistics, most notably reports about the gross domestic product (GDP) of the United States, as well as personal income, corporate profits and government spending|Large|Easy||
|[Yahoo Finance](https://finance.yahoo.com/)|Yahoo|Financial news, data and commentary including stock quotes, press releases, financial reports|Very Large|Very Easy|Here's the [S&P 500](https://finance.yahoo.com/quote/%5EGSPC/)|
|[Economic Research at the St. Louis Fed](https://fred.stlouisfed.org/)|St. Louis Fed|Money & Banking, Population, Employment, Production, Prices, International Data, Academic data (including the NBER Macrohistory database)|Very Large<br /><br />509,000 US and international time series from 87 sources|Very Easy<br /><br />Check out their [categories](https://fred.stlouisfed.org/categories) for a breakdown of their datasets||
